{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import model, sample, encoder\n",
    "from load_dataset import load_dataset, Sampler\n",
    "from accumulate import AccumulatingOptimizer\n",
    "import memory_saving_gradients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "SAMPLE_DIR = 'samples'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def maketree(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def randomize(context, hparams, p):\n",
    "    if p > 0:\n",
    "        mask = tf.random.uniform(shape=tf.shape(context)) < p\n",
    "        noise = tf.random.uniform(shape=tf.shape(context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)\n",
    "        return tf.where(mask, noise, context)\n",
    "    else:\n",
    "        return context\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class args_passed():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'wine_data.npz'\n",
    "        self.model_name = '117M'\n",
    "        self.combine = 50000\n",
    "        self.encoding = 'utf-8'\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 0.00002\n",
    "        self.accumulate_gradients = 1\n",
    "        self.memory_saving_gradients = False\n",
    "        self.only_train_transformer_layers = False\n",
    "        self.optimizer = 'adam'\n",
    "        self.noise = 0.0\n",
    "        \n",
    "        self.top_k = 40\n",
    "        self.top_p = 0.0\n",
    "        \n",
    "        self.restore_from = 'latest'\n",
    "        self.run_name = 'run1'\n",
    "        self.sample_every = 10\n",
    "        self.sample_length = 1023\n",
    "        self.sample_num = 1\n",
    "        self.save_every = 5\n",
    "        \n",
    "        self.val_dataset = None\n",
    "        self.val_batch_size = 2\n",
    "        self.val_batch_count = 40\n",
    "        self.val_every = 0\n",
    "\n",
    "args = args_passed()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def main():\n",
    "    enc = encoder.get_encoder(args.model_name)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join('models', args.model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if args.sample_length > hparams.n_ctx:\n",
    "        raise ValueError(\n",
    "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    if args.model_name == '345M':\n",
    "        args.memory_saving_gradients = True\n",
    "        if args.optimizer == 'adam':\n",
    "            args.only_train_transformer_layers = True\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    with tf.Session(config=config) as sess:\n",
    "        context = tf.placeholder(tf.int32, [args.batch_size, None])\n",
    "        context_in = randomize(context, hparams, args.noise)\n",
    "        output = model.model(hparams=hparams, X=context_in)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])\n",
    "            val_output = model.model(hparams=hparams, X=val_context)\n",
    "            val_loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))\n",
    "            val_loss_summary = tf.summary.scalar('val_loss', val_loss)\n",
    "\n",
    "\n",
    "        tf_sample = sample.sample_sequence(\n",
    "            hparams=hparams,\n",
    "            length=args.sample_length,\n",
    "            context=context,\n",
    "            batch_size=args.batch_size,\n",
    "            temperature=1.0,\n",
    "            top_k=args.top_k,\n",
    "            top_p=args.top_p)\n",
    "\n",
    "        all_vars = [v for v in tf.trainable_variables() if 'model' in v.name]\n",
    "        train_vars = [v for v in all_vars if '/h' in v.name] if args.only_train_transformer_layers else all_vars\n",
    "\n",
    "        if args.optimizer == 'adam':\n",
    "            opt = tf.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
    "        elif args.optimizer == 'sgd':\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)\n",
    "        else:\n",
    "            exit('Bad optimizer:', args.optimizer)\n",
    "\n",
    "        if args.accumulate_gradients > 1:\n",
    "            if args.memory_saving_gradients:\n",
    "                exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
    "            opt = AccumulatingOptimizer(\n",
    "                opt=opt,\n",
    "                var_list=train_vars)\n",
    "            opt_reset = opt.reset()\n",
    "            opt_compute = opt.compute_gradients(loss)\n",
    "            opt_apply = opt.apply_gradients()\n",
    "            summary_loss = tf.summary.scalar('loss', opt_apply)\n",
    "        else:\n",
    "            if args.memory_saving_gradients:\n",
    "                opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
    "            else:\n",
    "                opt_grads = tf.gradients(loss, train_vars)\n",
    "            opt_grads = list(zip(opt_grads, train_vars))\n",
    "            opt_apply = opt.apply_gradients(opt_grads)\n",
    "            summary_loss = tf.summary.scalar('loss', loss)\n",
    "\n",
    "        summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)\n",
    "        summaries = tf.summary.merge([summary_lr, summary_loss])\n",
    "\n",
    "        summary_log = tf.summary.FileWriter(\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "\n",
    "        saver = tf.train.Saver(\n",
    "            var_list=all_vars,\n",
    "            max_to_keep=5,\n",
    "            keep_checkpoint_every_n_hours=2)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if args.restore_from == 'latest':\n",
    "            ckpt = tf.train.latest_checkpoint(\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "            if ckpt is None:\n",
    "                # Get fresh GPT weights if new run.\n",
    "                ckpt = tf.train.latest_checkpoint(\n",
    "                    os.path.join('models', args.model_name))\n",
    "        elif args.restore_from == 'fresh':\n",
    "            ckpt = tf.train.latest_checkpoint(\n",
    "                os.path.join('models', args.model_name))\n",
    "        else:\n",
    "            ckpt = tf.train.latest_checkpoint(args.restore_from)\n",
    "        print('Loading checkpoint', ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        print('Loading dataset...')\n",
    "        chunks = load_dataset(enc, args.dataset, args.combine, encoding=args.encoding)\n",
    "        data_sampler = Sampler(chunks)\n",
    "        if args.val_every > 0:\n",
    "            if args.val_dataset:\n",
    "                val_chunks = load_dataset(enc, args.val_dataset, args.combine, encoding=args.encoding)\n",
    "            else:\n",
    "                val_chunks = chunks\n",
    "        print('dataset has', data_sampler.total_size, 'tokens')\n",
    "        print('Training...')\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            # Sample from validation set once with fixed seed to make\n",
    "            # it deterministic during training as well as across runs.\n",
    "            val_data_sampler = Sampler(val_chunks, seed=1)\n",
    "            val_batches = [[val_data_sampler.sample(1024) for _ in range(args.val_batch_size)]\n",
    "                           for _ in range(args.val_batch_count)]\n",
    "\n",
    "        counter = 1\n",
    "        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
    "        if os.path.exists(counter_path):\n",
    "            # Load the step number if we're resuming a run\n",
    "            # Add 1 so we don't immediately try to save again\n",
    "            with open(counter_path, 'r') as fp:\n",
    "                counter = int(fp.read()) + 1\n",
    "\n",
    "        def save():\n",
    "            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "            print(\n",
    "                'Saving',\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name,\n",
    "                             'model-{}').format(counter))\n",
    "            saver.save(\n",
    "                sess,\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
    "                global_step=counter)\n",
    "            with open(counter_path, 'w') as fp:\n",
    "                fp.write(str(counter) + '\\n')\n",
    "\n",
    "        def generate_samples():\n",
    "            print('Generating samples...')\n",
    "            context_tokens = data_sampler.sample(1)\n",
    "            all_text = []\n",
    "            index = 0\n",
    "            while index < args.sample_num:\n",
    "                out = sess.run(\n",
    "                    tf_sample,\n",
    "                    feed_dict={context: args.batch_size * [context_tokens]})\n",
    "                for i in range(min(args.sample_num - index, args.batch_size)):\n",
    "                    text = enc.decode(out[i])\n",
    "                    text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
    "                        index + 1, text)\n",
    "                    all_text.append(text)\n",
    "                    index += 1\n",
    "            print(text)\n",
    "            maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
    "            with open(\n",
    "                    os.path.join(SAMPLE_DIR, args.run_name,\n",
    "                                 'samples-{}').format(counter), 'w', encoding=args.encoding) as fp:\n",
    "                fp.write('\\n'.join(all_text))\n",
    "\n",
    "        def validation():\n",
    "            print('Calculating validation loss...')\n",
    "            losses = []\n",
    "            for batch in tqdm.tqdm(val_batches):\n",
    "                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
    "            v_val_loss = np.mean(losses)\n",
    "            v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})\n",
    "            summary_log.add_summary(v_summary, counter)\n",
    "            summary_log.flush()\n",
    "            print(\n",
    "                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
    "                .format(\n",
    "                    counter=counter,\n",
    "                    time=time.time() - start_time,\n",
    "                    loss=v_val_loss))\n",
    "\n",
    "        def sample_batch():\n",
    "            return [data_sampler.sample(1024) for _ in range(args.batch_size)]\n",
    "\n",
    "\n",
    "        avg_loss = (0.0, 0.0)\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                if counter % args.save_every == 0:\n",
    "                    save()\n",
    "                if counter % args.sample_every == 0:\n",
    "                    generate_samples()\n",
    "                if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
    "                    validation()\n",
    "\n",
    "                if args.accumulate_gradients > 1:\n",
    "                    sess.run(opt_reset)\n",
    "                    for _ in range(args.accumulate_gradients):\n",
    "                        sess.run(\n",
    "                            opt_compute, feed_dict={context: sample_batch()})\n",
    "                    (v_loss, v_summary) = sess.run((opt_apply, summaries))\n",
    "                else:\n",
    "                    (_, v_loss, v_summary) = sess.run(\n",
    "                        (opt_apply, loss, summaries),\n",
    "                        feed_dict={context: sample_batch()})\n",
    "\n",
    "                summary_log.add_summary(v_summary, counter)\n",
    "\n",
    "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
    "                            avg_loss[1] * 0.99 + 1.0)\n",
    "\n",
    "                print(\n",
    "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
    "                    .format(\n",
    "                        counter=counter,\n",
    "                        time=time.time() - start_time,\n",
    "                        loss=v_loss,\n",
    "                        avg=avg_loss[0] / avg_loss[1]))\n",
    "                \n",
    "                counter += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted')\n",
    "            save()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\tonylizj\\Documents\\GitHub\\HTV_Wine\\gpt-2-finetuning\\sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "Loading checkpoint models\\117M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\117M\\model.ckpt\n",
      "Loading dataset...\n",
      "dataset has 19312498 tokens\n",
      "Training...\n",
      "[1 | 9.79] loss=2.67 avg=2.67\n",
      "[2 | 16.69] loss=2.64 avg=2.65\n",
      "[3 | 23.55] loss=2.53 avg=2.61\n",
      "[4 | 30.51] loss=2.21 avg=2.51\n",
      "[5 | 37.80] loss=2.34 avg=2.47\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}